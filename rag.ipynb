{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4329e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c170abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23968\\3149388274.py:5: DeprecationWarning: The 'max_tokens' parameter is deprecated and will be removed in a future version. Please use 'max_completion_tokens' instead.\n",
      "  llm = ChatNVIDIA(\n"
     ]
    }
   ],
   "source": [
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\"\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=\"openai/gpt-oss-120b\", \n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d61834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalScore(BaseModel):\n",
    "    faithfulness: int = Field(description=\"Score 1-5: Does the answer stay true to context?\")\n",
    "    relevance: int = Field(description=\"Score 1-5: Does the answer address the question?\")\n",
    "    reasoning: str = Field(description=\"Reason for the scores\")\n",
    "    \n",
    "class GuardrailOutput(BaseModel):\n",
    "    is_relevant: bool = Field(description=\"Is the question about the document context?\")\n",
    "    reasoning: str = Field(description=\"Brief reason\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b269fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a security filter. Determine if this question is about contract analysis.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "guardrail_chain = guardrail_prompt | llm.with_structured_output(GuardrailOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4ef50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_uploaded_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    loader = PyPDFLoader(file_path) if ext == '.pdf' else Docx2txtLoader(file_path)\n",
    "    \n",
    "    splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(loader.load())\n",
    "    \n",
    "    # Force a fresh collection to avoid data mixing\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embedder,\n",
    "        collection_name=\"contract_store\"\n",
    "    )\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ec841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(retriever):\n",
    "    contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a contract assistant. Use the context to answer the question.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "    ])\n",
    "\n",
    "    return (\n",
    "        RunnableParallel({\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"history\": itemgetter(\"history\")\n",
    "        })\n",
    "        | contextual_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c251d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a prompt that explicitly asks for JSON\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "your task is to evaluate the following RAG response:\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Answer: \n",
    "{answer}\n",
    "\n",
    "Respond ONLY with a valid JSON object in this exact format:\n",
    "{{\n",
    "  \"faithfulness\": <int 1-5>,\n",
    "  \"relevance\": <int 1-5>,\n",
    "  \"reasoning\": \"<string>\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Simple chain — no with_structured_output\n",
    "eval_chain = eval_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887961d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Q&A ---\n",
      "Q: What are the main experience highlights?\n",
      "A: **Main Experience Highlights**\n",
      "\n",
      "| Area | Highlights |\n",
      "|------|------------|\n",
      "| **Training & Certifications** | • NVIDIA DLI Generative AI (Beginner) – ITI, Dec 2025  <br>• “Building RAG Agents with LLMs” – NVIDIA, Nov 2025 |\n",
      "| **Key Projects** | **MedAI – Intelligent Medical Assistant**  <br>• Lead AI Architect & Backend Engineer <br>• Designed a state‑machine‑driven agent orchestration layer with **LangGraph** <br>• Built a “Medical Gatekeeper” supervisor for intent classification, safety filtering and routing to RAG, web‑search, or LLM pathways <br>• Implemented a high‑precision RAG pipeline: semantic chunking + **ChromaDB** for medical PDF retrieval <br>• Integrated real‑time web search via **Tavily API** with time‑range filters for up‑to‑date clinical guidelines <br>• Ensured session persistence and fault‑tolerance with **Redis**‑backed storage across distributed nodes <br>• Tech stack: Python, LangChain, LangGraph, OpenAI GPT‑4, ChromaDB, Tavily, Redis, PostgreSQL <br><br>**Autonomous Vehicle Object Detection**  <br>• Multi‑class detection system using **YOLOv8** for vehicle & pedestrian perception <br>• Delivered end‑to‑end deployment via **Streamlit**, integrated OpenCV and CUDA‑optimized inference <br>• Managed model tracking & experiment logging with **MLflow** |\n",
      "| **Professional Experience** | **AI Training & Evaluation Specialist (Freelance – Alignerr)**  <br>• Conducted deep error analysis of autonomous agents, validated function‑calling logic, and improved multi‑step reasoning reliability <br>• Performed alignment audits by comparing recursive agent outputs with corrected references, boosting decision‑making consistency (08/2025‑Present) <br><br>**NLP Engineering Intern (Elevvo)**  <br>• Developed BERT/T5‑based QA and summarization models, optimizing ROUGE & F1 metrics <br>• Built semantic search and NER pipelines using Sentence‑Transformers & SpaCy for automated ranking and entity extraction (07/2025‑08/2025) <br><br>**Machine Learning Engineering Intern (Digital Egypt Pioneers Initiative – DEPI)**  <br>• Applied transfer learning for NLP & computer‑vision tasks, improving model accuracy and deployment speed |\n",
      "| **Leadership & Collaboration** | • Sole architect of the MedAI agent orchestration and RAG pipeline; mentored teammate on database design, session management, and front‑end integration <br>• Coordinated multi‑model team for autonomous‑vehicle platform, handling model training and unified deployment interface |\n",
      "| **Technical Proficiencies** | Python, LangChain, LangGraph, OpenAI GPT‑4, ChromaDB, Tavily API, Redis, PostgreSQL, YOLOv8, Streamlit, OpenCV, CUDA, MLflow, BERT/T5, Sentence‑Transformers, SpaCy |\n",
      "| **Languages** | Arabic (native), English (fluent) |\n",
      "\n",
      "These points capture the most significant achievements, responsibilities, and technical expertise demonstrated across your training, projects, and work experience.\n",
      "\n",
      "--- EVALUATION ---\n",
      "Faithfulness: 4/5\n",
      "Relevance:     5/5\n",
      "Reasoning:     The response accurately captures the majority of the information from the provided context, including training, project details, work experience, technical stack, and languages. It organizes the data clearly and reflects the key responsibilities and contributions. However, it introduces an element not present in the source (MLflow for model tracking) and omits a specific frontend technology (Chainlit), indicating a slight deviation from strict faithfulness. The content is highly relevant to a likely request for a concise summary of the candidate's experience.\n"
     ]
    }
   ],
   "source": [
    "retriever = process_uploaded_file(r\"C:\\Users\\user\\Downloads\\MarcelenoEskanderCV (5).pdf\")\n",
    "\n",
    "test_input = {\n",
    "    \"question\": \"What are the main experience highlights?\",\n",
    "    \"history\": []\n",
    "}\n",
    "\n",
    "retrieval_output = RunnableParallel({\n",
    "    \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\\n\".join(d.page_content for d in docs)),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}).invoke(test_input)\n",
    "\n",
    "answer = get_rag_chain(retriever).invoke(test_input)\n",
    "\n",
    "if not answer:\n",
    "    print(\"❌ Error: The RAG chain failed to generate an answer.\")\n",
    "else:\n",
    "    try:\n",
    "        response = eval_chain.invoke({\n",
    "            \"context\": retrieval_output[\"context\"],\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        \n",
    "        evaluation = EvalScore(**json.loads(response))\n",
    "\n",
    "        print(f\"--- Q&A ---\\nQ: {test_input['question']}\\nA: {answer}\\n\")\n",
    "        print(f\"--- EVALUATION ---\")\n",
    "        print(f\"Faithfulness: {evaluation.faithfulness}/5\")\n",
    "        print(f\"Relevance:     {evaluation.relevance}/5\")\n",
    "        print(f\"Reasoning:     {evaluation.reasoning}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Evaluation Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
